{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## еще можно open, promo, SchoolHoliday сделать dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmspe(y,y_pred):\n",
    "    summ = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] != 0:\n",
    "            summ += (1-y_pred[i]/y[i])**2\n",
    "    return sqrt(summ/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/train_v2.csv')\n",
    "df_train['Date']  = pd.to_datetime(df_train['Date'], errors='coerce')\n",
    "df_train['Year'] = df_train['Date'].dt.year\n",
    "df_train['Month'] = df_train['Date'].dt.month\n",
    "df_train['WeekOfYear'] = df_train['Date'].dt.weekofyear\n",
    "df_train['Day'] = df_train['Date'].dt.day\n",
    "df_train = df_train.drop('Date', axis=1)\n",
    "\n",
    "# df_train = df_train[df_train[\"Open\"] != 0]\n",
    "# df_train = df_train[df_train[\"Sales\"] > 0]\n",
    "\n",
    "df_test = pd.read_csv('data/test_v2.csv')\n",
    "df_test['Date']  = pd.to_datetime(df_test['Date'], errors='coerce')\n",
    "df_test['Year'] = df_test['Date'].dt.year\n",
    "df_test['Month'] = df_test['Date'].dt.month\n",
    "df_test['WeekOfYear'] = df_test['Date'].dt.weekofyear\n",
    "df_test['Day'] = df_test['Date'].dt.day\n",
    "df_test = df_test.drop('Date', axis=1)\n",
    "\n",
    "df_store = pd.read_csv('data/store.csv')\n",
    "df_store['CompetitionDistance'].fillna(-1,inplace=True)\n",
    "df_store['CompetitionOpenSinceMonth'].fillna(-1,inplace=True)\n",
    "df_store['CompetitionOpenSinceYear'].fillna(-1,inplace=True)\n",
    "df_store['Promo2SinceWeek'].fillna(0,inplace=True)\n",
    "df_store['Promo2SinceYear'].fillna(0,inplace=True)\n",
    "df_store['PromoInterval'].fillna(0,inplace=True)\n",
    "\n",
    "df_train_store = df_train.join(df_store.set_index('Store'), on='Store')\n",
    "df_train_store['CompetitionOpen'] = 12 * (df_train_store.Year - df_train_store.CompetitionOpenSinceYear) + \\\n",
    "        (df_train_store.Month - df_train_store.CompetitionOpenSinceMonth)\n",
    "df_train_store['CompetitionOpen'] = df_train_store.CompetitionOpen.apply(lambda x: x if x > 0 else 0)\n",
    "df_train_store.loc[df_train_store.CompetitionOpenSinceYear < 0, 'CompetitionOpen'] = 0\n",
    "df_train_store['PromoOpen'] = 12 * (df_train_store.Year - df_train_store.Promo2SinceYear) + \\\n",
    "        (df_train_store.WeekOfYear - df_train_store.Promo2SinceWeek) / 4.0\n",
    "df_train_store['PromoOpen'] = df_train_store.PromoOpen.apply(lambda x: x if x > 0 else 0)\n",
    "df_train_store.loc[df_train_store.Promo2SinceYear == 0, 'PromoOpen'] = 0\n",
    "df_train_store.drop(['CompetitionOpenSinceYear',\n",
    "                     'CompetitionOpenSinceMonth',\n",
    "                     'Promo2SinceYear',\n",
    "                     'Promo2SinceWeek'], axis=1, inplace=True)\n",
    "df_train_store.drop(['Promo2','PromoInterval'], axis=1, inplace=True)\n",
    "df_train_store.drop(['Year','Month'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "df_test_store = df_test.join(df_store.set_index('Store'), on='Store')\n",
    "df_test_store['CompetitionOpen'] = 12 * (df_test_store.Year - df_test_store.CompetitionOpenSinceYear) + \\\n",
    "        (df_test_store.Month - df_test_store.CompetitionOpenSinceMonth)\n",
    "df_test_store['CompetitionOpen'] = df_test_store.CompetitionOpen.apply(lambda x: x if x > 0 else 0)\n",
    "df_test_store.loc[df_test_store.CompetitionOpenSinceYear < 0, 'CompetitionOpen'] = 0\n",
    "df_test_store['PromoOpen'] = 12 * (df_test_store.Year - df_test_store.Promo2SinceYear) + \\\n",
    "        (df_test_store.WeekOfYear - df_test_store.Promo2SinceWeek) / 4.0\n",
    "df_test_store['PromoOpen'] = df_test_store.PromoOpen.apply(lambda x: x if x > 0 else 0)\n",
    "df_test_store.loc[df_test_store.Promo2SinceYear == 0, 'PromoOpen'] = 0\n",
    "df_test_store.drop(['CompetitionOpenSinceYear',\n",
    "                     'CompetitionOpenSinceMonth',\n",
    "                     'Promo2SinceYear',\n",
    "                     'Promo2SinceWeek'], axis=1, inplace=True)\n",
    "df_test_store.drop(['Promo2','PromoInterval'], axis=1, inplace=True)\n",
    "df_test_store.drop(['Year','Month'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "df = df_train_store\n",
    "labels = df['Sales']\n",
    "df.drop(['Sales'], axis=1, inplace=True)\n",
    "df_train_store_with_out_sales = df\n",
    "frames = [df_train_store_with_out_sales, df_test_store]\n",
    "super_df = pd.concat(frames)\n",
    "\n",
    "mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n",
    "super_df.StateHoliday.replace(mappings, inplace=True)\n",
    "super_df.StoreType.replace(mappings, inplace=True)\n",
    "super_df.Assortment.replace(mappings, inplace=True)\n",
    "\n",
    "# scaler = MinMaxScaler() \n",
    "# scaled_values = scaler.fit_transform(super_df) \n",
    "# super_df.iloc[:,2] = scaled_values[:,2]\n",
    "# super_df.iloc[:,11:14] = scaled_values[:,11:14]\n",
    "\n",
    "super_df = pd.get_dummies(super_df, columns=['Store', \n",
    "                                                         'DayOfWeek',\n",
    "                                                         'Open',\n",
    "                                                         'Promo',\n",
    "                                                         'SchoolHoliday',\n",
    "                                                         'StateHoliday',\n",
    "                                                         'WeekOfYear',\n",
    "                                                         'Day',\n",
    "                                                         'StoreType',\n",
    "                                                         'Assortment'])\n",
    "\n",
    "start_ind = df_train_store.shape[0]\n",
    "df_train_store = super_df.iloc[:start_ind,:]\n",
    "df_test_store = super_df.iloc[start_ind:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customers</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>CompetitionOpen</th>\n",
       "      <th>PromoOpen</th>\n",
       "      <th>Store_1</th>\n",
       "      <th>Store_2</th>\n",
       "      <th>Store_3</th>\n",
       "      <th>Store_4</th>\n",
       "      <th>Store_5</th>\n",
       "      <th>Store_6</th>\n",
       "      <th>...</th>\n",
       "      <th>Day_29</th>\n",
       "      <th>Day_30</th>\n",
       "      <th>Day_31</th>\n",
       "      <th>StoreType_1</th>\n",
       "      <th>StoreType_2</th>\n",
       "      <th>StoreType_3</th>\n",
       "      <th>StoreType_4</th>\n",
       "      <th>Assortment_1</th>\n",
       "      <th>Assortment_2</th>\n",
       "      <th>Assortment_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>616</td>\n",
       "      <td>1270.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>624</td>\n",
       "      <td>570.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>58.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>678</td>\n",
       "      <td>14130.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>45.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1632</td>\n",
       "      <td>620.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>617</td>\n",
       "      <td>29910.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customers  CompetitionDistance  CompetitionOpen  PromoOpen  Store_1  \\\n",
       "0        616               1270.0             76.0       0.00        1   \n",
       "1        624                570.0             86.0      58.00        0   \n",
       "2        678              14130.0             97.0      45.75        0   \n",
       "3       1632                620.0             64.0       0.00        0   \n",
       "4        617              29910.0              0.0       0.00        0   \n",
       "\n",
       "   Store_2  Store_3  Store_4  Store_5  Store_6      ...       Day_29  Day_30  \\\n",
       "0        0        0        0        0        0      ...            0       1   \n",
       "1        1        0        0        0        0      ...            0       1   \n",
       "2        0        1        0        0        0      ...            0       1   \n",
       "3        0        0        1        0        0      ...            0       1   \n",
       "4        0        0        0        1        0      ...            0       1   \n",
       "\n",
       "   Day_31  StoreType_1  StoreType_2  StoreType_3  StoreType_4  Assortment_1  \\\n",
       "0       0            0            0            1            0             1   \n",
       "1       0            1            0            0            0             1   \n",
       "2       0            1            0            0            0             1   \n",
       "3       0            0            0            1            0             0   \n",
       "4       0            1            0            0            0             1   \n",
       "\n",
       "   Assortment_2  Assortment_3  \n",
       "0             0             0  \n",
       "1             0             0  \n",
       "2             0             0  \n",
       "3             0             1  \n",
       "4             0             0  \n",
       "\n",
       "[5 rows x 1188 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>CompetitionOpen</th>\n",
       "      <th>PromoOpen</th>\n",
       "      <th>Store_1</th>\n",
       "      <th>Store_2</th>\n",
       "      <th>Store_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Day_29</th>\n",
       "      <th>Day_30</th>\n",
       "      <th>Day_31</th>\n",
       "      <th>StoreType_1</th>\n",
       "      <th>StoreType_2</th>\n",
       "      <th>StoreType_3</th>\n",
       "      <th>StoreType_4</th>\n",
       "      <th>Assortment_1</th>\n",
       "      <th>Assortment_2</th>\n",
       "      <th>Assortment_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.111850</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016754</td>\n",
       "      <td>0.059163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125957</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007527</td>\n",
       "      <td>0.066378</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.165457</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.074315</td>\n",
       "      <td>0.725694</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.301894</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008186</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.112656</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.394287</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1185 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customers  Open  Promo  SchoolHoliday  CompetitionDistance  \\\n",
       "0   0.111850     1      1              1             0.016754   \n",
       "1   0.125957     1      1              1             0.007527   \n",
       "2   0.165457     1      1              1             0.186275   \n",
       "3   0.301894     1      1              1             0.008186   \n",
       "4   0.112656     1      1              1             0.394287   \n",
       "\n",
       "   CompetitionOpen  PromoOpen  Store_1  Store_2  Store_3      ...       \\\n",
       "0         0.059163   0.000000        1        0        0      ...        \n",
       "1         0.066378   0.895833        0        1        0      ...        \n",
       "2         0.074315   0.725694        0        0        1      ...        \n",
       "3         0.050505   0.000000        0        0        0      ...        \n",
       "4         0.002165   0.000000        0        0        0      ...        \n",
       "\n",
       "   Day_29  Day_30  Day_31  StoreType_1  StoreType_2  StoreType_3  StoreType_4  \\\n",
       "0       0       0       1            0            0            1            0   \n",
       "1       0       0       1            1            0            0            0   \n",
       "2       0       0       1            1            0            0            0   \n",
       "3       0       0       1            0            0            1            0   \n",
       "4       0       0       1            1            0            0            0   \n",
       "\n",
       "   Assortment_1  Assortment_2  Assortment_3  \n",
       "0             1             0             0  \n",
       "1             1             0             0  \n",
       "2             1             0             0  \n",
       "3             0             0             1  \n",
       "4             1             0             0  \n",
       "\n",
       "[5 rows x 1185 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_store.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = df_train_store.values\n",
    "y = labels.values\n",
    "\n",
    "X_test = df_test_store.values\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.85, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=1188, kernel_initializer=\"normal\", activation=\"relu\", units=150)`\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"normal\", units=1)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/anaconda3/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56865 samples, validate on 10035 samples\n",
      "Epoch 1/200\n",
      "56865/56865 [==============================] - 2s - loss: 55348696.6067 - val_loss: 49596182.8520\n",
      "Epoch 2/200\n",
      "56865/56865 [==============================] - 1s - loss: 45244700.6384 - val_loss: 41198808.9537\n",
      "Epoch 3/200\n",
      "56865/56865 [==============================] - 1s - loss: 40118256.9503 - val_loss: 37198436.7324\n",
      "Epoch 4/200\n",
      "56865/56865 [==============================] - 1s - loss: 35462653.0692 - val_loss: 31770426.5237\n",
      "Epoch 5/200\n",
      "56865/56865 [==============================] - 2s - loss: 29251021.0977 - val_loss: 24857340.0299\n",
      "Epoch 6/200\n",
      "56865/56865 [==============================] - 2s - loss: 22317472.5506 - val_loss: 17495676.9671\n",
      "Epoch 7/200\n",
      "56865/56865 [==============================] - 2s - loss: 15388143.6848 - val_loss: 11169319.5092\n",
      "Epoch 8/200\n",
      "56865/56865 [==============================] - 1s - loss: 10006191.3379 - val_loss: 6777096.3850\n",
      "Epoch 9/200\n",
      "56865/56865 [==============================] - 1s - loss: 6673750.1033 - val_loss: 4433861.6233\n",
      "Epoch 10/200\n",
      "56865/56865 [==============================] - 1s - loss: 4982878.7181 - val_loss: 3462412.4868\n",
      "Epoch 11/200\n",
      "56865/56865 [==============================] - 2s - loss: 4279340.1360 - val_loss: 3116152.2053\n",
      "Epoch 12/200\n",
      "56865/56865 [==============================] - 1s - loss: 4025667.5364 - val_loss: 2989254.7302\n",
      "Epoch 13/200\n",
      "56865/56865 [==============================] - 1s - loss: 3887465.2476 - val_loss: 2940483.6639\n",
      "Epoch 14/200\n",
      "56865/56865 [==============================] - 1s - loss: 3809045.5703 - val_loss: 2913372.9641\n",
      "Epoch 15/200\n",
      "56865/56865 [==============================] - 1s - loss: 3740560.0960 - val_loss: 2880122.3832\n",
      "Epoch 16/200\n",
      "56865/56865 [==============================] - 1s - loss: 3718155.7644 - val_loss: 2847441.2220\n",
      "Epoch 17/200\n",
      "56865/56865 [==============================] - 1s - loss: 3727556.1120 - val_loss: 2828910.6706\n",
      "Epoch 18/200\n",
      "56865/56865 [==============================] - 1s - loss: 3667856.6964 - val_loss: 2787498.1567\n",
      "Epoch 19/200\n",
      "56865/56865 [==============================] - 1s - loss: 3632259.6668 - val_loss: 2759497.3313\n",
      "Epoch 20/200\n",
      "56865/56865 [==============================] - 1s - loss: 3640394.7226 - val_loss: 2736063.5126\n",
      "Epoch 21/200\n",
      "56865/56865 [==============================] - 1s - loss: 3556140.7363 - val_loss: 2708501.2710\n",
      "Epoch 22/200\n",
      "56865/56865 [==============================] - 1s - loss: 3487381.8651 - val_loss: 2698953.4582\n",
      "Epoch 23/200\n",
      "56865/56865 [==============================] - 1s - loss: 3546118.9410 - val_loss: 2679729.4035\n",
      "Epoch 24/200\n",
      "56865/56865 [==============================] - 1s - loss: 3549205.8399 - val_loss: 2657210.3357\n",
      "Epoch 25/200\n",
      "56865/56865 [==============================] - 1s - loss: 3564978.3877 - val_loss: 2647379.8665\n",
      "Epoch 26/200\n",
      "56865/56865 [==============================] - 1s - loss: 3502770.0223 - val_loss: 2649291.0382\n",
      "Epoch 27/200\n",
      "56865/56865 [==============================] - 1s - loss: 3444429.9308 - val_loss: 2639172.2935\n",
      "Epoch 28/200\n",
      "56865/56865 [==============================] - 1s - loss: 3475624.7968 - val_loss: 2630958.7636\n",
      "Epoch 29/200\n",
      "56865/56865 [==============================] - 1s - loss: 3494357.6595 - val_loss: 2620098.6697\n",
      "Epoch 30/200\n",
      "56865/56865 [==============================] - 1s - loss: 3469734.1222 - val_loss: 2601154.9444\n",
      "Epoch 31/200\n",
      "56865/56865 [==============================] - 1s - loss: 3475815.5398 - val_loss: 2583826.6691\n",
      "Epoch 32/200\n",
      "56865/56865 [==============================] - 1s - loss: 3443544.6104 - val_loss: 2591088.2764\n",
      "Epoch 33/200\n",
      "56865/56865 [==============================] - 1s - loss: 3443108.0484 - val_loss: 2577015.9539\n",
      "Epoch 34/200\n",
      "56865/56865 [==============================] - 1s - loss: 3470679.6441 - val_loss: 2571341.6331\n",
      "Epoch 35/200\n",
      "56865/56865 [==============================] - 1s - loss: 3440260.8795 - val_loss: 2548053.8985\n",
      "Epoch 36/200\n",
      "56865/56865 [==============================] - 1s - loss: 3409700.4638 - val_loss: 2541166.1023\n",
      "Epoch 37/200\n",
      "56865/56865 [==============================] - 1s - loss: 3389153.9500 - val_loss: 2537026.7559\n",
      "Epoch 38/200\n",
      "56865/56865 [==============================] - 1s - loss: 3390232.5619 - val_loss: 2526292.4062\n",
      "Epoch 39/200\n",
      "56865/56865 [==============================] - 1s - loss: 3398422.2657 - val_loss: 2515101.2643\n",
      "Epoch 40/200\n",
      "56865/56865 [==============================] - 1s - loss: 3385841.3359 - val_loss: 2515087.0806\n",
      "Epoch 41/200\n",
      "56865/56865 [==============================] - 1s - loss: 3402143.6019 - val_loss: 2527038.4356\n",
      "Epoch 42/200\n",
      "56865/56865 [==============================] - 1s - loss: 3398451.1495 - val_loss: 2515970.4256\n",
      "Epoch 43/200\n",
      "56865/56865 [==============================] - 1s - loss: 3348373.4971 - val_loss: 2497156.3245\n",
      "Epoch 44/200\n",
      "56865/56865 [==============================] - 2s - loss: 3344665.1175 - val_loss: 2487423.2827\n",
      "Epoch 45/200\n",
      "56865/56865 [==============================] - 1s - loss: 3373588.3851 - val_loss: 2483962.1941\n",
      "Epoch 46/200\n",
      "56865/56865 [==============================] - 1s - loss: 3340742.8339 - val_loss: 2471953.8112\n",
      "Epoch 47/200\n",
      "56865/56865 [==============================] - 1s - loss: 3388130.3357 - val_loss: 2475040.9652\n",
      "Epoch 48/200\n",
      "56865/56865 [==============================] - 1s - loss: 3350194.9824 - val_loss: 2462227.9524\n",
      "Epoch 49/200\n",
      "56865/56865 [==============================] - 1s - loss: 3368353.0074 - val_loss: 2468751.6430\n",
      "Epoch 50/200\n",
      "56865/56865 [==============================] - 1s - loss: 3361572.1686 - val_loss: 2458379.2115\n",
      "Epoch 51/200\n",
      "56865/56865 [==============================] - 1s - loss: 3371016.5780 - val_loss: 2449708.8834\n",
      "Epoch 52/200\n",
      "56865/56865 [==============================] - 1s - loss: 3272809.2269 - val_loss: 2469308.7820\n",
      "Epoch 53/200\n",
      "56865/56865 [==============================] - 1s - loss: 3321579.5441 - val_loss: 2446024.5950\n",
      "Epoch 54/200\n",
      "56865/56865 [==============================] - 1s - loss: 3256734.8646 - val_loss: 2436381.6759\n",
      "Epoch 55/200\n",
      "56865/56865 [==============================] - 1s - loss: 3303517.6750 - val_loss: 2427252.8002\n",
      "Epoch 56/200\n",
      "56865/56865 [==============================] - 1s - loss: 3350871.4898 - val_loss: 2434261.9409\n",
      "Epoch 57/200\n",
      "56865/56865 [==============================] - 1s - loss: 3283465.3204 - val_loss: 2461823.0646\n",
      "Epoch 58/200\n",
      "56865/56865 [==============================] - 1s - loss: 3277599.9696 - val_loss: 2412441.0561\n",
      "Epoch 59/200\n",
      "56865/56865 [==============================] - 1s - loss: 3296138.6370 - val_loss: 2413158.3142\n",
      "Epoch 60/200\n",
      "56865/56865 [==============================] - 1s - loss: 3290469.1587 - val_loss: 2398261.0348\n",
      "Epoch 61/200\n",
      "56865/56865 [==============================] - 1s - loss: 3257522.0519 - val_loss: 2401428.8960\n",
      "Epoch 62/200\n",
      "56865/56865 [==============================] - 1s - loss: 3282590.2134 - val_loss: 2393603.3099\n",
      "Epoch 63/200\n",
      "56865/56865 [==============================] - 1s - loss: 3233169.3930 - val_loss: 2393259.4425\n",
      "Epoch 64/200\n",
      "56865/56865 [==============================] - 1s - loss: 3234276.4498 - val_loss: 2386170.1994\n",
      "Epoch 65/200\n",
      "56865/56865 [==============================] - 1s - loss: 3242070.3570 - val_loss: 2379109.3541\n",
      "Epoch 66/200\n",
      "56865/56865 [==============================] - 1s - loss: 3211408.7639 - val_loss: 2384403.9084\n",
      "Epoch 67/200\n",
      "56865/56865 [==============================] - 1s - loss: 3199419.8853 - val_loss: 2362654.6183\n",
      "Epoch 68/200\n",
      "56865/56865 [==============================] - 1s - loss: 3226958.5994 - val_loss: 2362326.3557\n",
      "Epoch 69/200\n",
      "56865/56865 [==============================] - 1s - loss: 3226974.2907 - val_loss: 2362315.4486\n",
      "Epoch 70/200\n",
      "56865/56865 [==============================] - 1s - loss: 3209390.5118 - val_loss: 2368977.4140\n",
      "Epoch 71/200\n",
      "56865/56865 [==============================] - 1s - loss: 3183342.7945 - val_loss: 2352114.6338\n",
      "Epoch 72/200\n",
      "56865/56865 [==============================] - 1s - loss: 3251084.5963 - val_loss: 2350614.0791\n",
      "Epoch 73/200\n",
      "56865/56865 [==============================] - 1s - loss: 3214098.2145 - val_loss: 2332535.9656\n",
      "Epoch 74/200\n",
      "56865/56865 [==============================] - 1s - loss: 3164953.3395 - val_loss: 2331951.3509\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56865/56865 [==============================] - 1s - loss: 3158893.1251 - val_loss: 2333659.2846\n",
      "Epoch 76/200\n",
      "56865/56865 [==============================] - 1s - loss: 3149185.4146 - val_loss: 2345642.2245\n",
      "Epoch 77/200\n",
      "56865/56865 [==============================] - 1s - loss: 3159473.4194 - val_loss: 2312394.1155\n",
      "Epoch 78/200\n",
      "56865/56865 [==============================] - 1s - loss: 3149429.0390 - val_loss: 2327497.2484\n",
      "Epoch 79/200\n",
      "56865/56865 [==============================] - 1s - loss: 3129515.5286 - val_loss: 2310244.7964\n",
      "Epoch 80/200\n",
      "56865/56865 [==============================] - 1s - loss: 3194233.7221 - val_loss: 2305430.1758\n",
      "Epoch 81/200\n",
      "56865/56865 [==============================] - 1s - loss: 3157342.8121 - val_loss: 2307687.486265800.51\n",
      "Epoch 82/200\n",
      "56865/56865 [==============================] - 1s - loss: 3171670.7269 - val_loss: 2294590.4095\n",
      "Epoch 83/200\n",
      "56865/56865 [==============================] - 1s - loss: 3140624.5910 - val_loss: 2297319.2830\n",
      "Epoch 84/200\n",
      "56865/56865 [==============================] - 1s - loss: 3175376.8234 - val_loss: 2301989.8620\n",
      "Epoch 85/200\n",
      "56865/56865 [==============================] - 1s - loss: 3107009.2677 - val_loss: 2282660.6997\n",
      "Epoch 86/200\n",
      "56865/56865 [==============================] - 1s - loss: 3135184.4795 - val_loss: 2273016.8270\n",
      "Epoch 87/200\n",
      "56865/56865 [==============================] - 1s - loss: 3141473.1345 - val_loss: 2293476.9331\n",
      "Epoch 88/200\n",
      "56865/56865 [==============================] - 1s - loss: 3132715.5025 - val_loss: 2259505.4310\n",
      "Epoch 89/200\n",
      "56865/56865 [==============================] - 1s - loss: 3118797.0608 - val_loss: 2251154.3589\n",
      "Epoch 90/200\n",
      "56865/56865 [==============================] - 1s - loss: 3098625.5838 - val_loss: 2248705.4162\n",
      "Epoch 91/200\n",
      "56865/56865 [==============================] - 1s - loss: 3112780.3442 - val_loss: 2246374.2808\n",
      "Epoch 92/200\n",
      "56865/56865 [==============================] - 1s - loss: 3185833.3187 - val_loss: 2266612.8380\n",
      "Epoch 93/200\n",
      "56865/56865 [==============================] - 1s - loss: 3092761.8836 - val_loss: 2233955.8530\n",
      "Epoch 94/200\n",
      "56865/56865 [==============================] - 1s - loss: 3112688.1429 - val_loss: 2238384.7367\n",
      "Epoch 95/200\n",
      "56865/56865 [==============================] - 1s - loss: 3097348.2393 - val_loss: 2229764.4108\n",
      "Epoch 96/200\n",
      "56865/56865 [==============================] - 1s - loss: 3122654.3479 - val_loss: 2225761.4259\n",
      "Epoch 97/200\n",
      "56865/56865 [==============================] - 1s - loss: 3080932.5328 - val_loss: 2215495.0039\n",
      "Epoch 98/200\n",
      "56865/56865 [==============================] - 1s - loss: 3088315.1291 - val_loss: 2235052.5726\n",
      "Epoch 99/200\n",
      "56865/56865 [==============================] - 1s - loss: 3101593.3089 - val_loss: 2215384.5402\n",
      "Epoch 100/200\n",
      "56865/56865 [==============================] - 1s - loss: 3048820.1121 - val_loss: 2220502.3686\n",
      "Epoch 101/200\n",
      "56865/56865 [==============================] - 1s - loss: 3075270.4508 - val_loss: 2205028.4374\n",
      "Epoch 102/200\n",
      "56865/56865 [==============================] - 1s - loss: 3049349.9945 - val_loss: 2194256.1225\n",
      "Epoch 103/200\n",
      "56865/56865 [==============================] - 1s - loss: 3042201.4219 - val_loss: 2197206.1621\n",
      "Epoch 104/200\n",
      "56865/56865 [==============================] - 1s - loss: 3052354.7917 - val_loss: 2194966.3648\n",
      "Epoch 105/200\n",
      "56865/56865 [==============================] - 1s - loss: 3066207.8138 - val_loss: 2175704.2849\n",
      "Epoch 106/200\n",
      "56865/56865 [==============================] - 1s - loss: 3078187.1145 - val_loss: 2182063.7298\n",
      "Epoch 107/200\n",
      "56865/56865 [==============================] - 1s - loss: 2978465.1809 - val_loss: 2169715.9847\n",
      "Epoch 108/200\n",
      "56865/56865 [==============================] - 1s - loss: 3031226.7002 - val_loss: 2167880.1924\n",
      "Epoch 109/200\n",
      "56865/56865 [==============================] - 1s - loss: 3026044.0023 - val_loss: 2178357.9124\n",
      "Epoch 110/200\n",
      "56865/56865 [==============================] - 1s - loss: 3054229.1657 - val_loss: 2153834.8149\n",
      "Epoch 111/200\n",
      "56865/56865 [==============================] - 1s - loss: 3011987.0341 - val_loss: 2170007.6709\n",
      "Epoch 112/200\n",
      "56865/56865 [==============================] - 1s - loss: 3005667.4745 - val_loss: 2154601.1893\n",
      "Epoch 113/200\n",
      "56865/56865 [==============================] - 1s - loss: 3012378.6376 - val_loss: 2147464.9678\n",
      "Epoch 114/200\n",
      "56865/56865 [==============================] - 1s - loss: 2997558.8915 - val_loss: 2148944.5442\n",
      "Epoch 115/200\n",
      "56865/56865 [==============================] - 1s - loss: 3021491.2529 - val_loss: 2146959.1895\n",
      "Epoch 116/200\n",
      "56865/56865 [==============================] - 1s - loss: 3014201.9224 - val_loss: 2128631.4069\n",
      "Epoch 117/200\n",
      "56865/56865 [==============================] - 1s - loss: 2970487.5160 - val_loss: 2118448.9032\n",
      "Epoch 118/200\n",
      "56865/56865 [==============================] - 1s - loss: 2994222.1903 - val_loss: 2109526.8734\n",
      "Epoch 119/200\n",
      "56865/56865 [==============================] - 1s - loss: 2969375.8441 - val_loss: 2117308.0526\n",
      "Epoch 120/200\n",
      "56865/56865 [==============================] - 1s - loss: 2952875.4791 - val_loss: 2120155.8336\n",
      "Epoch 121/200\n",
      "56865/56865 [==============================] - 2s - loss: 2926507.7625 - val_loss: 2121490.3274\n",
      "Epoch 122/200\n",
      "56865/56865 [==============================] - 1s - loss: 2963122.6775 - val_loss: 2102270.3558\n",
      "Epoch 123/200\n",
      "56865/56865 [==============================] - 1s - loss: 2912688.5709 - val_loss: 2098718.4375\n",
      "Epoch 124/200\n",
      "56865/56865 [==============================] - 1s - loss: 2937554.2427 - val_loss: 2084911.3709\n",
      "Epoch 125/200\n",
      "56865/56865 [==============================] - 1s - loss: 2942744.1848 - val_loss: 2079392.43343049\n",
      "Epoch 126/200\n",
      "56865/56865 [==============================] - 1s - loss: 2953082.0027 - val_loss: 2074557.3240\n",
      "Epoch 127/200\n",
      "56865/56865 [==============================] - 1s - loss: 2947674.8293 - val_loss: 2068310.2195\n",
      "Epoch 128/200\n",
      "56865/56865 [==============================] - 1s - loss: 2913306.9878 - val_loss: 2078328.3622\n",
      "Epoch 129/200\n",
      "56865/56865 [==============================] - 1s - loss: 2863355.2440 - val_loss: 2066227.1657\n",
      "Epoch 130/200\n",
      "56865/56865 [==============================] - 1s - loss: 2903724.7489 - val_loss: 2057044.2609\n",
      "Epoch 131/200\n",
      "56865/56865 [==============================] - 1s - loss: 2919806.0510 - val_loss: 2056998.6246\n",
      "Epoch 132/200\n",
      "56865/56865 [==============================] - 1s - loss: 2926638.4053 - val_loss: 2060272.7125\n",
      "Epoch 133/200\n",
      "56865/56865 [==============================] - 1s - loss: 2896553.2965 - val_loss: 2041869.9673\n",
      "Epoch 134/200\n",
      "56865/56865 [==============================] - 1s - loss: 2872570.8335 - val_loss: 2047099.7491\n",
      "Epoch 135/200\n",
      "56865/56865 [==============================] - 1s - loss: 2900432.5210 - val_loss: 2031349.5048\n",
      "Epoch 136/200\n",
      "56865/56865 [==============================] - 1s - loss: 2863334.5297 - val_loss: 2031417.8234\n",
      "Epoch 137/200\n",
      "56865/56865 [==============================] - 1s - loss: 2846038.9336 - val_loss: 2030686.2011\n",
      "Epoch 138/200\n",
      "56865/56865 [==============================] - 1s - loss: 2858897.6645 - val_loss: 2023243.6060\n",
      "Epoch 139/200\n",
      "56865/56865 [==============================] - 1s - loss: 2864160.1165 - val_loss: 2002559.6614\n",
      "Epoch 140/200\n",
      "56865/56865 [==============================] - 1s - loss: 2855888.2523 - val_loss: 2018407.1789\n",
      "Epoch 141/200\n",
      "56865/56865 [==============================] - 1s - loss: 2843844.6424 - val_loss: 2000601.0154\n",
      "Epoch 142/200\n",
      "56865/56865 [==============================] - 1s - loss: 2878406.7692 - val_loss: 2003551.9776\n",
      "Epoch 143/200\n",
      "56865/56865 [==============================] - 1s - loss: 2840004.4783 - val_loss: 1991267.7967\n",
      "Epoch 144/200\n",
      "56865/56865 [==============================] - 1s - loss: 2831436.9205 - val_loss: 1977120.8095\n",
      "Epoch 145/200\n",
      "56865/56865 [==============================] - 1s - loss: 2810792.7209 - val_loss: 1980368.4163\n",
      "Epoch 146/200\n",
      "56865/56865 [==============================] - 1s - loss: 2841395.4496 - val_loss: 1976138.7159\n",
      "Epoch 147/200\n",
      "56865/56865 [==============================] - 1s - loss: 2837237.9330 - val_loss: 1963876.6720\n",
      "Epoch 148/200\n",
      "56865/56865 [==============================] - 1s - loss: 2805002.8060 - val_loss: 1956192.5590\n",
      "Epoch 149/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56865/56865 [==============================] - 1s - loss: 2825738.2662 - val_loss: 1953800.4101\n",
      "Epoch 150/200\n",
      "56865/56865 [==============================] - 1s - loss: 2811926.5582 - val_loss: 1962722.5254\n",
      "Epoch 151/200\n",
      "56865/56865 [==============================] - 1s - loss: 2807708.8846 - val_loss: 1948105.4188\n",
      "Epoch 152/200\n",
      "56865/56865 [==============================] - 1s - loss: 2799342.0332 - val_loss: 1945877.6725\n",
      "Epoch 153/200\n",
      "56865/56865 [==============================] - 1s - loss: 2773440.6771 - val_loss: 1939269.0896\n",
      "Epoch 154/200\n",
      "56865/56865 [==============================] - 1s - loss: 2796365.1032 - val_loss: 1936465.9361\n",
      "Epoch 155/200\n",
      "56865/56865 [==============================] - 1s - loss: 2748933.8331 - val_loss: 1928551.1755\n",
      "Epoch 156/200\n",
      "56865/56865 [==============================] - 1s - loss: 2757456.4387 - val_loss: 1921191.9406\n",
      "Epoch 157/200\n",
      "56865/56865 [==============================] - 2s - loss: 2784953.8425 - val_loss: 1917654.4350\n",
      "Epoch 158/200\n",
      "56865/56865 [==============================] - 2s - loss: 2791859.0173 - val_loss: 1901695.2930620\n",
      "Epoch 159/200\n",
      "56865/56865 [==============================] - 1s - loss: 2746076.9242 - val_loss: 1900079.3743\n",
      "Epoch 160/200\n",
      "56865/56865 [==============================] - 1s - loss: 2747161.2019 - val_loss: 1895131.5633\n",
      "Epoch 161/200\n",
      "56865/56865 [==============================] - 1s - loss: 2710921.4304 - val_loss: 1887688.5491\n",
      "Epoch 162/200\n",
      "56865/56865 [==============================] - 1s - loss: 2788576.8892 - val_loss: 1881679.8766\n",
      "Epoch 163/200\n",
      "56865/56865 [==============================] - 2s - loss: 2761457.0971 - val_loss: 1895812.1335\n",
      "Epoch 164/200\n",
      "56865/56865 [==============================] - 1s - loss: 2717713.3980 - val_loss: 1871934.8237\n",
      "Epoch 165/200\n",
      "56865/56865 [==============================] - 1s - loss: 2764229.3012 - val_loss: 1863980.1230\n",
      "Epoch 166/200\n",
      "56865/56865 [==============================] - 1s - loss: 2714507.3744 - val_loss: 1856230.0331\n",
      "Epoch 167/200\n",
      "56865/56865 [==============================] - 2s - loss: 2685906.5549 - val_loss: 1849739.5263\n",
      "Epoch 168/200\n",
      "56865/56865 [==============================] - 2s - loss: 2722594.5917 - val_loss: 1851247.0464\n",
      "Epoch 169/200\n",
      "56865/56865 [==============================] - 1s - loss: 2724141.9096 - val_loss: 1864455.9427\n",
      "Epoch 170/200\n",
      "56865/56865 [==============================] - 1s - loss: 2728231.0327 - val_loss: 1838288.1366\n",
      "Epoch 171/200\n",
      "56865/56865 [==============================] - 1s - loss: 2689344.9536 - val_loss: 1848332.7257\n",
      "Epoch 172/200\n",
      "56865/56865 [==============================] - 1s - loss: 2663801.4889 - val_loss: 1818275.0040\n",
      "Epoch 173/200\n",
      "56865/56865 [==============================] - 1s - loss: 2693651.0686 - val_loss: 1828392.7084\n",
      "Epoch 174/200\n",
      "56865/56865 [==============================] - 1s - loss: 2636708.7828 - val_loss: 1807409.4540\n",
      "Epoch 175/200\n",
      "56865/56865 [==============================] - 1s - loss: 2669758.7685 - val_loss: 1809352.3809\n",
      "Epoch 176/200\n",
      "56865/56865 [==============================] - 1s - loss: 2602387.9224 - val_loss: 1798155.8822\n",
      "Epoch 177/200\n",
      "56865/56865 [==============================] - 1s - loss: 2643765.5903 - val_loss: 1793222.6605\n",
      "Epoch 178/200\n",
      "56865/56865 [==============================] - 1s - loss: 2658190.4145 - val_loss: 1787719.5129\n",
      "Epoch 179/200\n",
      "56865/56865 [==============================] - 1s - loss: 2665504.2063 - val_loss: 1792390.4156\n",
      "Epoch 180/200\n",
      "56865/56865 [==============================] - 2s - loss: 2603515.0098 - val_loss: 1775935.9046\n",
      "Epoch 181/200\n",
      "56865/56865 [==============================] - 2s - loss: 2619755.2181 - val_loss: 1765791.2914\n",
      "Epoch 182/200\n",
      "56865/56865 [==============================] - 2s - loss: 2624794.6449 - val_loss: 1750182.1302\n",
      "Epoch 183/200\n",
      "56865/56865 [==============================] - 2s - loss: 2598984.0656 - val_loss: 1751628.6699\n",
      "Epoch 184/200\n",
      "56865/56865 [==============================] - 1s - loss: 2586256.6868 - val_loss: 1741560.0668\n",
      "Epoch 185/200\n",
      "56865/56865 [==============================] - 1s - loss: 2603960.6889 - val_loss: 1735549.7605\n",
      "Epoch 186/200\n",
      "56865/56865 [==============================] - 1s - loss: 2574804.5701 - val_loss: 1724029.9202\n",
      "Epoch 187/200\n",
      "56865/56865 [==============================] - 2s - loss: 2563242.9119 - val_loss: 1743722.2054\n",
      "Epoch 188/200\n",
      "56865/56865 [==============================] - 1s - loss: 2588211.6136 - val_loss: 1722522.3164\n",
      "Epoch 189/200\n",
      "56865/56865 [==============================] - 1s - loss: 2556477.3683 - val_loss: 1714642.8187\n",
      "Epoch 190/200\n",
      "56865/56865 [==============================] - 1s - loss: 2578427.9379 - val_loss: 1716018.3464\n",
      "Epoch 191/200\n",
      "56865/56865 [==============================] - 1s - loss: 2550723.3588 - val_loss: 1692872.1699\n",
      "Epoch 192/200\n",
      "56865/56865 [==============================] - 1s - loss: 2544712.2653 - val_loss: 1691174.2092\n",
      "Epoch 193/200\n",
      "56865/56865 [==============================] - 1s - loss: 2568145.3750 - val_loss: 1679754.8175\n",
      "Epoch 194/200\n",
      "56865/56865 [==============================] - 1s - loss: 2522317.0619 - val_loss: 1666777.2800\n",
      "Epoch 195/200\n",
      "56865/56865 [==============================] - 2s - loss: 2534974.1858 - val_loss: 1669223.424440042.455\n",
      "Epoch 196/200\n",
      "56865/56865 [==============================] - 1s - loss: 2492913.0211 - val_loss: 1652737.8380\n",
      "Epoch 197/200\n",
      "56865/56865 [==============================] - 1s - loss: 2513682.0927 - val_loss: 1647293.0167\n",
      "Epoch 198/200\n",
      "56865/56865 [==============================] - 1s - loss: 2495545.5929 - val_loss: 1645835.8575\n",
      "Epoch 199/200\n",
      "56865/56865 [==============================] - 1s - loss: 2506021.2859 - val_loss: 1640106.3066\n",
      "Epoch 200/200\n",
      "56865/56865 [==============================] - 1s - loss: 2496291.9179 - val_loss: 1642295.0329\n",
      "---------//---------\n",
      "output      150\n",
      "batch size  2000\n",
      "time taken  359.09401392936707\n",
      "E_out is    0.14360691599628572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_dims = [150]\n",
    "bss = [2000]\n",
    "num_epochs = 200\n",
    "verb = 1\n",
    "for out_dim in out_dims:\n",
    "    for bs in bss:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(output_dim=out_dim, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(output_dim=1, kernel_initializer='normal'))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        model.fit(X_train, y_train, \n",
    "        verbose=verb, \n",
    "        nb_epoch=num_epochs, \n",
    "        batch_size=bs,\n",
    "        validation_data=(X_validation, y_validation))\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        pred = model.predict(X_validation)\n",
    "        accuracy = rmspe(y_validation, pred)\n",
    "        print('---------//---------')\n",
    "        print('output     ',out_dim)\n",
    "        print('batch size ',bs)\n",
    "        print('time taken ',end-start)\n",
    "        print('E_out is   ',accuracy)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(X_test)\n",
    "pred_test = np.reshape(pred_test, pred_test.shape[0])\n",
    "submission['Sales'] = pred_test\n",
    "cols = ['Id','Sales']\n",
    "submission['Id'] = submission.index + 1\n",
    "submission = submission[cols]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
