{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## убрать строки с нулевыми значениями Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmspe(y,y_pred):\n",
    "    summ = 0\n",
    "    for i in range(len(y)):\n",
    "        if y.iloc[i] != 0:\n",
    "            summ += (1-y_pred[i]/y.iloc[i])**2\n",
    "    return sqrt(summ/len(y))\n",
    "\n",
    "def substract_cols(df):\n",
    "    df['CompetitionOpen'] = 12 * (df.Year - df['CompetitionOpenSinceYear']) + (df.Month - df['CompetitionOpenSinceMonth'])\n",
    "    df['CompetitionOpen'] = df.CompetitionOpen.apply(lambda x: x if x > 0 else 0)\n",
    "    df.loc[df['CompetitionOpenSinceYear'] < 0, 'CompetitionOpen'] = 0\n",
    "    df['PromoOpen'] = 12 * (df.Year - df.Promo2SinceYear) + (df['WeekOfYear'] - df['Promo2SinceWeek']) / 4.0\n",
    "    df['PromoOpen'] = df.PromoOpen.apply(lambda x: x if x > 0 else 0)\n",
    "    df.loc[df['Promo2SinceYear'] == 0, 'PromoOpen'] = 0\n",
    "    df.loc[df['Promo2SinceYear'] < 0, 'PromoOpen'] = 0\n",
    "    df.drop(['CompetitionOpenSinceYear',\n",
    "             'CompetitionOpenSinceMonth',\n",
    "             'Promo2SinceYear',\n",
    "             'Promo2SinceWeek'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/train_v2.csv')\n",
    "df_train['Date']  = pd.to_datetime(df_train['Date'], errors='coerce')\n",
    "df_train['Year'] = df_train['Date'].dt.year\n",
    "df_train['Month'] = df_train['Date'].dt.month\n",
    "df_train['WeekOfYear'] = df_train['Date'].dt.weekofyear\n",
    "df_train['Day'] = df_train['Date'].dt.day\n",
    "df_train = df_train.drop('Date', axis=1)\n",
    "\n",
    "df_test = pd.read_csv('data/test_v2.csv')\n",
    "df_test['Date']  = pd.to_datetime(df_test['Date'], errors='coerce')\n",
    "df_test['Year'] = df_test['Date'].dt.year\n",
    "df_test['Month'] = df_test['Date'].dt.month\n",
    "df_test['WeekOfYear'] = df_test['Date'].dt.weekofyear\n",
    "df_test['Day'] = df_test['Date'].dt.day\n",
    "df_test = df_test.drop('Date', axis=1)\n",
    "\n",
    "df_store = pd.read_csv('data/store.csv')\n",
    "df_store = df_store.fillna(-1)\n",
    "\n",
    "df_train = df_train[df_train.Sales > 0]\n",
    "df_train = df_train[df_train.Customers > 0]\n",
    "\n",
    "df_train_store = df_train.join(df_store.set_index('Store'), on='Store')\n",
    "df_train_store = substract_cols(df_train_store)\n",
    "df_train_store.drop(['Open','PromoInterval','Year','Month','WeekOfYear'], axis=1, inplace=True)\n",
    "\n",
    "df_test_store = df_test.join(df_store.set_index('Store'), on='Store')\n",
    "df_test_store = substract_cols(df_test_store)\n",
    "df_test_store.drop(['Open','PromoInterval','Year','Month','WeekOfYear'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df = df_train_store\n",
    "labels = df['Sales']\n",
    "df.drop(['Sales'], axis=1, inplace=True)\n",
    "df_train_store_with_out_sales = df\n",
    "frames = [df_train_store_with_out_sales, df_test_store]\n",
    "super_df = pd.concat(frames)\n",
    "\n",
    "mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n",
    "super_df.StateHoliday.replace(mappings, inplace=True)\n",
    "super_df.StoreType.replace(mappings, inplace=True)\n",
    "super_df.Assortment.replace(mappings, inplace=True)\n",
    "\n",
    "# scaler = MinMaxScaler() \n",
    "# scaled_values = scaler.fit_transform(super_df) \n",
    "# super_df.iloc[:,2] = scaled_values[:,2]\n",
    "# super_df.iloc[:,11:14] = scaled_values[:,11:14]\n",
    "\n",
    "super_df = pd.get_dummies(super_df, columns=['Store', \n",
    "                                                         'DayOfWeek',\n",
    "                                                         'StateHoliday',\n",
    "                                                         'Day',\n",
    "                                                         'StoreType',\n",
    "                                                         'Assortment'])\n",
    "\n",
    "start_ind = df_train_store.shape[0]\n",
    "df_train_store = super_df.iloc[:start_ind,:]\n",
    "df_test_store = super_df.iloc[start_ind:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customers</th>\n",
       "      <th>Promo</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>Promo2</th>\n",
       "      <th>CompetitionOpen</th>\n",
       "      <th>PromoOpen</th>\n",
       "      <th>Store_1</th>\n",
       "      <th>Store_2</th>\n",
       "      <th>Store_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Day_29</th>\n",
       "      <th>Day_30</th>\n",
       "      <th>Day_31</th>\n",
       "      <th>StoreType_1</th>\n",
       "      <th>StoreType_2</th>\n",
       "      <th>StoreType_3</th>\n",
       "      <th>StoreType_4</th>\n",
       "      <th>Assortment_1</th>\n",
       "      <th>Assortment_2</th>\n",
       "      <th>Assortment_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>616</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1270.0</td>\n",
       "      <td>0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>624</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>1</td>\n",
       "      <td>86.0</td>\n",
       "      <td>58.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>678</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14130.0</td>\n",
       "      <td>1</td>\n",
       "      <td>97.0</td>\n",
       "      <td>45.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1632</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>620.0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>617</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29910.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customers  Promo  SchoolHoliday  CompetitionDistance  Promo2  \\\n",
       "0        616      1              0               1270.0       0   \n",
       "1        624      1              0                570.0       1   \n",
       "2        678      1              0              14130.0       1   \n",
       "3       1632      1              0                620.0       0   \n",
       "4        617      1              0              29910.0       0   \n",
       "\n",
       "   CompetitionOpen  PromoOpen  Store_1  Store_2  Store_3      ...       \\\n",
       "0             76.0       0.00        1        0        0      ...        \n",
       "1             86.0      58.00        0        1        0      ...        \n",
       "2             97.0      45.75        0        0        1      ...        \n",
       "3             64.0       0.00        0        0        0      ...        \n",
       "4              0.0       0.00        0        0        0      ...        \n",
       "\n",
       "   Day_29  Day_30  Day_31  StoreType_1  StoreType_2  StoreType_3  StoreType_4  \\\n",
       "0       0       1       0            0            0            1            0   \n",
       "1       0       1       0            1            0            0            0   \n",
       "2       0       1       0            1            0            0            0   \n",
       "3       0       1       0            0            0            1            0   \n",
       "4       0       1       0            1            0            0            0   \n",
       "\n",
       "   Assortment_1  Assortment_2  Assortment_3  \n",
       "0             1             0             0  \n",
       "1             1             0             0  \n",
       "2             1             0             0  \n",
       "3             0             0             1  \n",
       "4             1             0             0  \n",
       "\n",
       "[5 rows x 1170 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customers</th>\n",
       "      <th>Promo</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>Promo2</th>\n",
       "      <th>CompetitionOpen</th>\n",
       "      <th>PromoOpen</th>\n",
       "      <th>Store_1</th>\n",
       "      <th>Store_2</th>\n",
       "      <th>Store_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Day_29</th>\n",
       "      <th>Day_30</th>\n",
       "      <th>Day_31</th>\n",
       "      <th>StoreType_1</th>\n",
       "      <th>StoreType_2</th>\n",
       "      <th>StoreType_3</th>\n",
       "      <th>StoreType_4</th>\n",
       "      <th>Assortment_1</th>\n",
       "      <th>Assortment_2</th>\n",
       "      <th>Assortment_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>555</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1270.0</td>\n",
       "      <td>0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>625</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>570.0</td>\n",
       "      <td>1</td>\n",
       "      <td>92.0</td>\n",
       "      <td>64.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>821</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14130.0</td>\n",
       "      <td>1</td>\n",
       "      <td>103.0</td>\n",
       "      <td>52.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1498</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>620.0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>559</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29910.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customers  Promo  SchoolHoliday  CompetitionDistance  Promo2  \\\n",
       "0        555      1              1               1270.0       0   \n",
       "1        625      1              1                570.0       1   \n",
       "2        821      1              1              14130.0       1   \n",
       "3       1498      1              1                620.0       0   \n",
       "4        559      1              1              29910.0       0   \n",
       "\n",
       "   CompetitionOpen  PromoOpen  Store_1  Store_2  Store_3      ...       \\\n",
       "0             82.0       0.00        1        0        0      ...        \n",
       "1             92.0      64.50        0        1        0      ...        \n",
       "2            103.0      52.25        0        0        1      ...        \n",
       "3             70.0       0.00        0        0        0      ...        \n",
       "4              3.0       0.00        0        0        0      ...        \n",
       "\n",
       "   Day_29  Day_30  Day_31  StoreType_1  StoreType_2  StoreType_3  StoreType_4  \\\n",
       "0       0       0       1            0            0            1            0   \n",
       "1       0       0       1            1            0            0            0   \n",
       "2       0       0       1            1            0            0            0   \n",
       "3       0       0       1            0            0            1            0   \n",
       "4       0       0       1            1            0            0            0   \n",
       "\n",
       "   Assortment_1  Assortment_2  Assortment_3  \n",
       "0             1             0             0  \n",
       "1             1             0             0  \n",
       "2             1             0             0  \n",
       "3             0             0             1  \n",
       "4             1             0             0  \n",
       "\n",
       "[5 rows x 1170 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_store.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = df_train_store.values\n",
    "y = labels.values\n",
    "\n",
    "X_test = df_test_store.values\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.85, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=1170, kernel_initializer=\"normal\", activation=\"relu\", units=150)`\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"normal\", units=1)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/anaconda3/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54688 samples, validate on 8204 samples\n",
      "Epoch 1/200\n",
      "54688/54688 [==============================] - 3s - loss: 64445329.6068 - val_loss: 57123793.9444\n",
      "Epoch 2/200\n",
      "54688/54688 [==============================] - 2s - loss: 52876874.8964 - val_loss: 47793297.4705\n",
      "Epoch 3/200\n",
      "54688/54688 [==============================] - 2s - loss: 47131580.7150 - val_loss: 44021951.5085\n",
      "Epoch 4/200\n",
      "54688/54688 [==============================] - 2s - loss: 42874952.1521 - val_loss: 39162448.1307\n",
      "Epoch 5/200\n",
      "54688/54688 [==============================] - 2s - loss: 37085075.0884 - val_loss: 32874355.5748\n",
      "Epoch 6/200\n",
      "54688/54688 [==============================] - 2s - loss: 30302356.3043 - val_loss: 25686028.3374\n",
      "Epoch 7/200\n",
      "54688/54688 [==============================] - 2s - loss: 22879512.2592 - val_loss: 18560865.1438\n",
      "Epoch 8/200\n",
      "54688/54688 [==============================] - 2s - loss: 16271133.4930 - val_loss: 12558474.0590\n",
      "Epoch 9/200\n",
      "54688/54688 [==============================] - 2s - loss: 11029717.2601 - val_loss: 8198953.1728\n",
      "Epoch 10/200\n",
      "54688/54688 [==============================] - 2s - loss: 7552397.2659 - val_loss: 5668092.6299\n",
      "Epoch 11/200\n",
      "54688/54688 [==============================] - 2s - loss: 5881366.7891 - val_loss: 4495813.2999\n",
      "Epoch 12/200\n",
      "54688/54688 [==============================] - 2s - loss: 5251477.3629 - val_loss: 4047243.8708\n",
      "Epoch 13/200\n",
      "54688/54688 [==============================] - 2s - loss: 5027931.0547 - val_loss: 3891874.5907\n",
      "Epoch 14/200\n",
      "54688/54688 [==============================] - 2s - loss: 4921443.9690 - val_loss: 3829306.3225\n",
      "Epoch 15/200\n",
      "54688/54688 [==============================] - 2s - loss: 4874665.6761 - val_loss: 3803272.9949\n",
      "Epoch 16/200\n",
      "54688/54688 [==============================] - 2s - loss: 4856543.1456 - val_loss: 3781242.6936\n",
      "Epoch 17/200\n",
      "54688/54688 [==============================] - 2s - loss: 4868893.2610 - val_loss: 3758677.0383\n",
      "Epoch 18/200\n",
      "54688/54688 [==============================] - 2s - loss: 4853214.7254 - val_loss: 3748374.0051\n",
      "Epoch 19/200\n",
      "54688/54688 [==============================] - 2s - loss: 4836694.3223 - val_loss: 3712541.8957\n",
      "Epoch 20/200\n",
      "54688/54688 [==============================] - 2s - loss: 4790161.9370 - val_loss: 3693792.7128\n",
      "Epoch 21/200\n",
      "54688/54688 [==============================] - 2s - loss: 4714499.7582 - val_loss: 3670271.4707\n",
      "Epoch 22/200\n",
      "54688/54688 [==============================] - 2s - loss: 4729398.7011 - val_loss: 3629290.5000\n",
      "Epoch 23/200\n",
      "54688/54688 [==============================] - 3s - loss: 4673654.6780 - val_loss: 3598324.8986\n",
      "Epoch 24/200\n",
      "54688/54688 [==============================] - 3s - loss: 4633933.9315 - val_loss: 3548780.3993\n",
      "Epoch 25/200\n",
      "54688/54688 [==============================] - 3s - loss: 4646951.4054 - val_loss: 3510155.8913\n",
      "Epoch 26/200\n",
      "54688/54688 [==============================] - 3s - loss: 4647703.0713 - val_loss: 3478250.5936\n",
      "Epoch 27/200\n",
      "54688/54688 [==============================] - 3s - loss: 4548729.3325 - val_loss: 3434219.3176\n",
      "Epoch 28/200\n",
      "54688/54688 [==============================] - 2s - loss: 4619690.9577 - val_loss: 3410726.7296\n",
      "Epoch 29/200\n",
      "54688/54688 [==============================] - 2s - loss: 4491344.0829 - val_loss: 3342997.2640\n",
      "Epoch 30/200\n",
      "54688/54688 [==============================] - 2s - loss: 4524799.2852 - val_loss: 3322194.3161\n",
      "Epoch 31/200\n",
      "54688/54688 [==============================] - 2s - loss: 4442066.7005 - val_loss: 3295689.7875\n",
      "Epoch 32/200\n",
      "54688/54688 [==============================] - 3s - loss: 4438271.8816 - val_loss: 3261491.343146822.312 - ETA: 1s -\n",
      "Epoch 33/200\n",
      "54688/54688 [==============================] - 3s - loss: 4455626.9269 - val_loss: 3240258.8279\n",
      "Epoch 34/200\n",
      "54688/54688 [==============================] - 2s - loss: 4419454.3800 - val_loss: 3210339.0180\n",
      "Epoch 35/200\n",
      "54688/54688 [==============================] - 3s - loss: 4417991.9369 - val_loss: 3191712.7427\n",
      "Epoch 36/200\n",
      "54688/54688 [==============================] - 2s - loss: 4345692.6688 - val_loss: 3171815.4526\n",
      "Epoch 37/200\n",
      "54688/54688 [==============================] - 3s - loss: 4374351.5716 - val_loss: 3158012.0506\n",
      "Epoch 38/200\n",
      "54688/54688 [==============================] - 3s - loss: 4348642.7444 - val_loss: 3152599.5045\n",
      "Epoch 39/200\n",
      "54688/54688 [==============================] - 2s - loss: 4396704.7712 - val_loss: 3147930.5968\n",
      "Epoch 40/200\n",
      "54688/54688 [==============================] - 2s - loss: 4375381.7971 - val_loss: 3136299.0659\n",
      "Epoch 41/200\n",
      "54688/54688 [==============================] - 2s - loss: 4340137.9068 - val_loss: 3120374.4208\n",
      "Epoch 42/200\n",
      "54688/54688 [==============================] - 2s - loss: 4315049.9078 - val_loss: 3118734.7530\n",
      "Epoch 43/200\n",
      "54688/54688 [==============================] - 2s - loss: 4379148.1048 - val_loss: 3091210.8802\n",
      "Epoch 44/200\n",
      "54688/54688 [==============================] - 2s - loss: 4348546.3648 - val_loss: 3075801.8659\n",
      "Epoch 45/200\n",
      "54688/54688 [==============================] - 2s - loss: 4293835.9392 - val_loss: 3057932.6140\n",
      "Epoch 46/200\n",
      "54688/54688 [==============================] - 2s - loss: 4266506.0103 - val_loss: 3045302.8792\n",
      "Epoch 47/200\n",
      "54688/54688 [==============================] - 2s - loss: 4309407.7551 - val_loss: 3042093.0436\n",
      "Epoch 48/200\n",
      "54688/54688 [==============================] - 2s - loss: 4283661.9333 - val_loss: 3031974.3779\n",
      "Epoch 49/200\n",
      "54688/54688 [==============================] - 2s - loss: 4274590.0150 - val_loss: 3008206.4400\n",
      "Epoch 50/200\n",
      "54688/54688 [==============================] - 2s - loss: 4276366.2469 - val_loss: 3010249.9186\n",
      "Epoch 51/200\n",
      "54688/54688 [==============================] - 2s - loss: 4215009.0545 - val_loss: 2989649.1216\n",
      "Epoch 52/200\n",
      "54688/54688 [==============================] - 3s - loss: 4260270.0407 - val_loss: 2990876.0995\n",
      "Epoch 53/200\n",
      "54688/54688 [==============================] - 3s - loss: 4167027.8659 - val_loss: 2959539.9153\n",
      "Epoch 54/200\n",
      "54688/54688 [==============================] - 3s - loss: 4210363.1853 - val_loss: 2975525.8890\n",
      "Epoch 55/200\n",
      "54688/54688 [==============================] - 3s - loss: 4167754.2632 - val_loss: 2961510.8197\n",
      "Epoch 56/200\n",
      "54688/54688 [==============================] - 3s - loss: 4214600.7488 - val_loss: 2956132.7172\n",
      "Epoch 57/200\n",
      "54688/54688 [==============================] - 3s - loss: 4163574.2620 - val_loss: 2937973.5789\n",
      "Epoch 58/200\n",
      "54688/54688 [==============================] - 3s - loss: 4183411.1317 - val_loss: 2936016.2730\n",
      "Epoch 59/200\n",
      "54688/54688 [==============================] - 2s - loss: 4149388.5258 - val_loss: 2969491.9779\n",
      "Epoch 60/200\n",
      "54688/54688 [==============================] - 3s - loss: 4121837.8668 - val_loss: 2915968.7533\n",
      "Epoch 61/200\n",
      "54688/54688 [==============================] - 2s - loss: 4087988.2495 - val_loss: 2911655.4439\n",
      "Epoch 62/200\n",
      "54688/54688 [==============================] - 3s - loss: 4134159.2089 - val_loss: 2911675.2392\n",
      "Epoch 63/200\n",
      "54688/54688 [==============================] - 2s - loss: 4104562.6137 - val_loss: 2908381.1900 - ETA: 0s - loss: 4114806.0 - ETA: 0s - loss: 4105088.5\n",
      "Epoch 64/200\n",
      "54688/54688 [==============================] - 2s - loss: 4149270.0680 - val_loss: 2915985.6246\n",
      "Epoch 65/200\n",
      "54688/54688 [==============================] - 3s - loss: 4123815.4089 - val_loss: 2907954.2527\n",
      "Epoch 66/200\n",
      "54688/54688 [==============================] - 4s - loss: 4098555.5418 - val_loss: 2910785.3169\n",
      "Epoch 67/200\n",
      "54688/54688 [==============================] - 3s - loss: 4155653.2117 - val_loss: 2903298.6371\n",
      "Epoch 68/200\n",
      "54688/54688 [==============================] - 3s - loss: 4112186.3659 - val_loss: 2889553.4197\n",
      "Epoch 69/200\n",
      "54688/54688 [==============================] - 4s - loss: 4107809.2087 - val_loss: 2895789.6092\n",
      "Epoch 70/200\n",
      "54688/54688 [==============================] - 2s - loss: 4137641.1847 - val_loss: 2877072.3697\n",
      "Epoch 71/200\n",
      "54688/54688 [==============================] - 2s - loss: 4078589.8515 - val_loss: 2873920.00689420\n",
      "Epoch 72/200\n",
      "54688/54688 [==============================] - 3s - loss: 4025693.7757 - val_loss: 2870805.5957\n",
      "Epoch 73/200\n",
      "54688/54688 [==============================] - 3s - loss: 4093038.3750 - val_loss: 2880043.4927\n",
      "Epoch 74/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54688/54688 [==============================] - 3s - loss: 4079351.3388 - val_loss: 2863541.4169\n",
      "Epoch 75/200\n",
      "54688/54688 [==============================] - 2s - loss: 4039817.6085 - val_loss: 2852301.4892\n",
      "Epoch 76/200\n",
      "54688/54688 [==============================] - 2s - loss: 4045266.4262 - val_loss: 2862973.4887\n",
      "Epoch 77/200\n",
      "54688/54688 [==============================] - 3s - loss: 4112147.3746 - val_loss: 2844947.8397\n",
      "Epoch 78/200\n",
      "54688/54688 [==============================] - 2s - loss: 4094499.3412 - val_loss: 2848440.2964\n",
      "Epoch 79/200\n",
      "54688/54688 [==============================] - 2s - loss: 4003536.8297 - val_loss: 2848557.5889\n",
      "Epoch 80/200\n",
      "54688/54688 [==============================] - 2s - loss: 4086538.5338 - val_loss: 2848175.5963\n",
      "Epoch 81/200\n",
      "54688/54688 [==============================] - 3s - loss: 4075310.3385 - val_loss: 2854943.3543\n",
      "Epoch 82/200\n",
      "54688/54688 [==============================] - 3s - loss: 4059199.2374 - val_loss: 2838581.5190\n",
      "Epoch 83/200\n",
      "54688/54688 [==============================] - 2s - loss: 4025488.1112 - val_loss: 2824528.5283\n",
      "Epoch 84/200\n",
      "54688/54688 [==============================] - 2s - loss: 4043391.0925 - val_loss: 2815080.5544\n",
      "Epoch 85/200\n",
      "54688/54688 [==============================] - 2s - loss: 4046111.5803 - val_loss: 2822578.1915\n",
      "Epoch 86/200\n",
      "54688/54688 [==============================] - 2s - loss: 3995440.6990 - val_loss: 2815503.0150\n",
      "Epoch 87/200\n",
      "54688/54688 [==============================] - 2s - loss: 4053950.4231 - val_loss: 2821428.3876\n",
      "Epoch 88/200\n",
      "54688/54688 [==============================] - 2s - loss: 3952794.8462 - val_loss: 2809660.6958\n",
      "Epoch 89/200\n",
      "54688/54688 [==============================] - 2s - loss: 3947048.1963 - val_loss: 2803182.0408\n",
      "Epoch 90/200\n",
      "54688/54688 [==============================] - 2s - loss: 4060818.8471 - val_loss: 2802753.6941\n",
      "Epoch 91/200\n",
      "54688/54688 [==============================] - 3s - loss: 4017493.6292 - val_loss: 2800695.1140\n",
      "Epoch 92/200\n",
      "54688/54688 [==============================] - 3s - loss: 4042893.6727 - val_loss: 2804876.4650\n",
      "Epoch 93/200\n",
      "54688/54688 [==============================] - 2s - loss: 3977779.9721 - val_loss: 2787581.828382015.287\n",
      "Epoch 94/200\n",
      "54688/54688 [==============================] - 3s - loss: 3974153.5940 - val_loss: 2782870.2533\n",
      "Epoch 95/200\n",
      "54688/54688 [==============================] - 3s - loss: 3992458.0545 - val_loss: 2765450.2237\n",
      "Epoch 96/200\n",
      "54688/54688 [==============================] - 3s - loss: 3954043.2558 - val_loss: 2764984.7109\n",
      "Epoch 97/200\n",
      "54688/54688 [==============================] - 2s - loss: 3990392.7066 - val_loss: 2766723.6247\n",
      "Epoch 98/200\n",
      "54688/54688 [==============================] - 2s - loss: 3923853.0037 - val_loss: 2759337.8671\n",
      "Epoch 99/200\n",
      "54688/54688 [==============================] - 2s - loss: 3947277.0170 - val_loss: 2771535.8252\n",
      "Epoch 100/200\n",
      "54688/54688 [==============================] - 2s - loss: 3963661.6319 - val_loss: 2768667.351992781.50 - ETA: 0s - loss: 3983304. - ETA: 0s - loss: 3962162.1\n",
      "Epoch 101/200\n",
      "54688/54688 [==============================] - 2s - loss: 3923520.7793 - val_loss: 2746788.0407\n",
      "Epoch 102/200\n",
      "54688/54688 [==============================] - 2s - loss: 3961422.5911 - val_loss: 2741433.4882\n",
      "Epoch 103/200\n",
      "54688/54688 [==============================] - 2s - loss: 3971098.7707 - val_loss: 2737058.5143\n",
      "Epoch 104/200\n",
      "54688/54688 [==============================] - 2s - loss: 3924584.1496 - val_loss: 2729563.2203\n",
      "Epoch 105/200\n",
      "54688/54688 [==============================] - 2s - loss: 3908905.4274 - val_loss: 2729572.8876\n",
      "Epoch 106/200\n",
      "54688/54688 [==============================] - 2s - loss: 3906427.3822 - val_loss: 2734071.7189\n",
      "Epoch 107/200\n",
      "54688/54688 [==============================] - 2s - loss: 3936549.1340 - val_loss: 2701594.0235\n",
      "Epoch 108/200\n",
      "54688/54688 [==============================] - 2s - loss: 3877464.7287 - val_loss: 2713422.466225324.91 - ETA: 0s - loss: 383760\n",
      "Epoch 109/200\n",
      "54688/54688 [==============================] - 2s - loss: 3934521.2888 - val_loss: 2725868.6247\n",
      "Epoch 110/200\n",
      "54688/54688 [==============================] - 2s - loss: 3956184.5796 - val_loss: 2714093.517973011.519 - ETA: 1s - loss\n",
      "Epoch 111/200\n",
      "54688/54688 [==============================] - 2s - loss: 3923437.0998 - val_loss: 2703474.3904\n",
      "Epoch 112/200\n",
      "54688/54688 [==============================] - 2s - loss: 3899429.8206 - val_loss: 2690305.1225\n",
      "Epoch 113/200\n",
      "54688/54688 [==============================] - 2s - loss: 3936460.5547 - val_loss: 2678685.0847\n",
      "Epoch 114/200\n",
      "54688/54688 [==============================] - 2s - loss: 3865904.3588 - val_loss: 2684519.1312\n",
      "Epoch 115/200\n",
      "54688/54688 [==============================] - 2s - loss: 3851614.4459 - val_loss: 2684675.8195\n",
      "Epoch 116/200\n",
      "54688/54688 [==============================] - 2s - loss: 3859400.0806 - val_loss: 2665102.1464\n",
      "Epoch 117/200\n",
      "54688/54688 [==============================] - 2s - loss: 3865042.7034 - val_loss: 2680217.4356\n",
      "Epoch 118/200\n",
      "54688/54688 [==============================] - 2s - loss: 3841017.0005 - val_loss: 2673598.8063\n",
      "Epoch 119/200\n",
      "54688/54688 [==============================] - 2s - loss: 3859880.3690 - val_loss: 2662941.285193622 - ETA: 1s -\n",
      "Epoch 120/200\n",
      "54688/54688 [==============================] - 2s - loss: 3868551.6727 - val_loss: 2671323.4581\n",
      "Epoch 121/200\n",
      "54688/54688 [==============================] - 2s - loss: 3819405.3299 - val_loss: 2650496.0569\n",
      "Epoch 122/200\n",
      "54688/54688 [==============================] - 2s - loss: 3858646.4753 - val_loss: 2665457.4850\n",
      "Epoch 123/200\n",
      "54688/54688 [==============================] - 2s - loss: 3835974.8090 - val_loss: 2647373.614228700\n",
      "Epoch 124/200\n",
      "54688/54688 [==============================] - 2s - loss: 3843772.8303 - val_loss: 2643186.435240510.48\n",
      "Epoch 125/200\n",
      "54688/54688 [==============================] - 2s - loss: 3808198.7942 - val_loss: 2637062.4588\n",
      "Epoch 126/200\n",
      "54688/54688 [==============================] - 3s - loss: 3800319.6220 - val_loss: 2638401.9128\n",
      "Epoch 127/200\n",
      "54688/54688 [==============================] - 3s - loss: 3835662.1714 - val_loss: 2626050.0651\n",
      "Epoch 128/200\n",
      "54688/54688 [==============================] - 3s - loss: 3770231.4944 - val_loss: 2614862.6922\n",
      "Epoch 129/200\n",
      "54688/54688 [==============================] - 3s - loss: 3863738.9502 - val_loss: 2625027.0129\n",
      "Epoch 130/200\n",
      "54688/54688 [==============================] - 4s - loss: 3824512.4139 - val_loss: 2619108.4943\n",
      "Epoch 131/200\n",
      "54688/54688 [==============================] - 3s - loss: 3840119.4178 - val_loss: 2619831.3755\n",
      "Epoch 132/200\n",
      "54688/54688 [==============================] - 3s - loss: 3758648.5742 - val_loss: 2620475.8025\n",
      "Epoch 133/200\n",
      "54688/54688 [==============================] - 2s - loss: 3838591.6299 - val_loss: 2612312.1795\n",
      "Epoch 134/200\n",
      "54688/54688 [==============================] - 2s - loss: 3778718.0529 - val_loss: 2594476.1621\n",
      "Epoch 135/200\n",
      "54688/54688 [==============================] - 2s - loss: 3768754.5382 - val_loss: 2595684.1906\n",
      "Epoch 136/200\n",
      "54688/54688 [==============================] - 2s - loss: 3796121.9952 - val_loss: 2603660.6347\n",
      "Epoch 137/200\n",
      "54688/54688 [==============================] - 2s - loss: 3773021.8060 - val_loss: 2591120.9317\n",
      "Epoch 138/200\n",
      "54688/54688 [==============================] - 3s - loss: 3772151.0555 - val_loss: 2573924.4912\n",
      "Epoch 139/200\n",
      "54688/54688 [==============================] - 3s - loss: 3805234.9938 - val_loss: 2566850.3418\n",
      "Epoch 140/200\n",
      "54688/54688 [==============================] - 3s - loss: 3716926.2058 - val_loss: 2584122.6774\n",
      "Epoch 141/200\n",
      "54688/54688 [==============================] - 3s - loss: 3781896.6688 - val_loss: 2566337.0698\n",
      "Epoch 142/200\n",
      "54688/54688 [==============================] - 3s - loss: 3745999.8292 - val_loss: 2599838.9211\n",
      "Epoch 143/200\n",
      "54688/54688 [==============================] - 3s - loss: 3793647.7205 - val_loss: 2561466.3430\n",
      "Epoch 144/200\n",
      "54688/54688 [==============================] - 3s - loss: 3772147.3068 - val_loss: 2553667.0278\n",
      "Epoch 145/200\n",
      "54688/54688 [==============================] - 3s - loss: 3712588.4756 - val_loss: 2578916.2465\n",
      "Epoch 146/200\n",
      "54688/54688 [==============================] - 3s - loss: 3749751.5547 - val_loss: 2577984.4820\n",
      "Epoch 147/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54688/54688 [==============================] - 2s - loss: 3796688.4959 - val_loss: 2563248.0475\n",
      "Epoch 148/200\n",
      "54688/54688 [==============================] - 2s - loss: 3713737.7872 - val_loss: 2539912.7700\n",
      "Epoch 149/200\n",
      "54688/54688 [==============================] - 2s - loss: 3715525.0328 - val_loss: 2537003.0416\n",
      "Epoch 150/200\n",
      "54688/54688 [==============================] - 2s - loss: 3735215.2956 - val_loss: 2538659.2747\n",
      "Epoch 151/200\n",
      "54688/54688 [==============================] - 2s - loss: 3681819.2313 - val_loss: 2530759.6402\n",
      "Epoch 152/200\n",
      "54688/54688 [==============================] - 2s - loss: 3707540.5577 - val_loss: 2529780.9693\n",
      "Epoch 153/200\n",
      "54688/54688 [==============================] - 2s - loss: 3691708.6694 - val_loss: 2515718.7796\n",
      "Epoch 154/200\n",
      "54688/54688 [==============================] - 2s - loss: 3688532.2798 - val_loss: 2504788.0440\n",
      "Epoch 155/200\n",
      "54688/54688 [==============================] - 2s - loss: 3690753.4598 - val_loss: 2503744.0997\n",
      "Epoch 156/200\n",
      "54688/54688 [==============================] - 2s - loss: 3736473.0850 - val_loss: 2518077.9350\n",
      "Epoch 157/200\n",
      "54688/54688 [==============================] - 2s - loss: 3654723.4824 - val_loss: 2497858.2034\n",
      "Epoch 158/200\n",
      "54688/54688 [==============================] - 2s - loss: 3660565.2656 - val_loss: 2488330.7532\n",
      "Epoch 159/200\n",
      "54688/54688 [==============================] - 2s - loss: 3687266.7929 - val_loss: 2481209.4771\n",
      "Epoch 160/200\n",
      "54688/54688 [==============================] - 2s - loss: 3670544.2357 - val_loss: 2482817.9767\n",
      "Epoch 161/200\n",
      "54688/54688 [==============================] - 2s - loss: 3624469.8037 - val_loss: 2477988.7563\n",
      "Epoch 162/200\n",
      "54688/54688 [==============================] - 2s - loss: 3680177.1596 - val_loss: 2479653.762888903.9\n",
      "Epoch 163/200\n",
      "54688/54688 [==============================] - 2s - loss: 3636332.8499 - val_loss: 2466185.1953\n",
      "Epoch 164/200\n",
      "54688/54688 [==============================] - 2s - loss: 3627593.0676 - val_loss: 2456671.8614\n",
      "Epoch 165/200\n",
      "54688/54688 [==============================] - 2s - loss: 3642482.9895 - val_loss: 2483234.8233\n",
      "Epoch 166/200\n",
      "54688/54688 [==============================] - 2s - loss: 3578356.4993 - val_loss: 2444780.6539\n",
      "Epoch 167/200\n",
      "54688/54688 [==============================] - 2s - loss: 3602973.9015 - val_loss: 2441395.5082\n",
      "Epoch 168/200\n",
      "54688/54688 [==============================] - 2s - loss: 3635975.8401 - val_loss: 2432767.8342\n",
      "Epoch 169/200\n",
      "54688/54688 [==============================] - 2s - loss: 3631844.2643 - val_loss: 2413967.6316\n",
      "Epoch 170/200\n",
      "54688/54688 [==============================] - 2s - loss: 3604586.2327 - val_loss: 2427585.3023\n",
      "Epoch 171/200\n",
      "54688/54688 [==============================] - 2s - loss: 3561046.9075 - val_loss: 2409545.3457\n",
      "Epoch 172/200\n",
      "54688/54688 [==============================] - 2s - loss: 3596578.4520 - val_loss: 2405883.8535\n",
      "Epoch 173/200\n",
      "54688/54688 [==============================] - 2s - loss: 3633792.8281 - val_loss: 2416023.1677\n",
      "Epoch 174/200\n",
      "54688/54688 [==============================] - 2s - loss: 3538628.2953 - val_loss: 2396254.5385\n",
      "Epoch 175/200\n",
      "54688/54688 [==============================] - 4s - loss: 3569509.8839 - val_loss: 2418135.038351514. - ETA: 2s - loss: 3617 - ETA: 1s - loss\n",
      "Epoch 176/200\n",
      "54688/54688 [==============================] - 2s - loss: 3616971.8056 - val_loss: 2398292.8074340\n",
      "Epoch 177/200\n",
      "54688/54688 [==============================] - 2s - loss: 3505950.7424 - val_loss: 2391609.0016\n",
      "Epoch 178/200\n",
      "54688/54688 [==============================] - 3s - loss: 3574279.1066 - val_loss: 2384221.6415\n",
      "Epoch 179/200\n",
      "54688/54688 [==============================] - 3s - loss: 3540000.3321 - val_loss: 2382236.3745\n",
      "Epoch 180/200\n",
      "54688/54688 [==============================] - 3s - loss: 3534860.7523 - val_loss: 2371626.7338\n",
      "Epoch 181/200\n",
      "54688/54688 [==============================] - 2s - loss: 3475162.3756 - val_loss: 2365153.3485\n",
      "Epoch 182/200\n",
      "54688/54688 [==============================] - 4s - loss: 3512539.5982 - val_loss: 2357044.276190545\n",
      "Epoch 183/200\n",
      "54688/54688 [==============================] - 2s - loss: 3495943.6290 - val_loss: 2341114.7556\n",
      "Epoch 184/200\n",
      "54688/54688 [==============================] - 2s - loss: 3496656.1712 - val_loss: 2365599.7671\n",
      "Epoch 185/200\n",
      "54688/54688 [==============================] - 2s - loss: 3482303.5102 - val_loss: 2334307.5001\n",
      "Epoch 186/200\n",
      "54688/54688 [==============================] - 2s - loss: 3516025.2642 - val_loss: 2319013.3623\n",
      "Epoch 187/200\n",
      "54688/54688 [==============================] - 2s - loss: 3468986.6181 - val_loss: 2315943.4982\n",
      "Epoch 188/200\n",
      "54688/54688 [==============================] - 3s - loss: 3502866.6044 - val_loss: 2327749.6086\n",
      "Epoch 189/200\n",
      "54688/54688 [==============================] - 3s - loss: 3470907.9149 - val_loss: 2324602.5385\n",
      "Epoch 190/200\n",
      "54688/54688 [==============================] - 4s - loss: 3446129.7826 - val_loss: 2312874.3844\n",
      "Epoch 191/200\n",
      "54688/54688 [==============================] - 3s - loss: 3468214.7131 - val_loss: 2305195.5006\n",
      "Epoch 192/200\n",
      "54688/54688 [==============================] - 3s - loss: 3427057.6193 - val_loss: 2301690.4320\n",
      "Epoch 193/200\n",
      "54688/54688 [==============================] - 3s - loss: 3447058.2260 - val_loss: 2265274.0316\n",
      "Epoch 194/200\n",
      "54688/54688 [==============================] - 3s - loss: 3402337.4095 - val_loss: 2267900.9037\n",
      "Epoch 195/200\n",
      "54688/54688 [==============================] - 3s - loss: 3449326.7849 - val_loss: 2264432.6688\n",
      "Epoch 196/200\n",
      "54688/54688 [==============================] - 3s - loss: 3466272.9186 - val_loss: 2253489.5779\n",
      "Epoch 197/200\n",
      "54688/54688 [==============================] - 3s - loss: 3396385.8514 - val_loss: 2243569.5301\n",
      "Epoch 198/200\n",
      "54688/54688 [==============================] - 3s - loss: 3383444.9797 - val_loss: 2231582.9949\n",
      "Epoch 199/200\n",
      "54688/54688 [==============================] - 2s - loss: 3367325.5916 - val_loss: 2230672.9437\n",
      "Epoch 200/200\n",
      "54688/54688 [==============================] - 2s - loss: 3371222.8358 - val_loss: 2228566.7852\n",
      "---------//---------\n",
      "output      150\n",
      "batch size  2000\n",
      "epochs  200\n",
      "RMSPE is  0.12058630107581389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_dims = [150]\n",
    "bss = [2000]\n",
    "num_epochs = 200\n",
    "dropout = 0.2\n",
    "verb = 1\n",
    "for out_dim in out_dims:\n",
    "    for bs in bss:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(output_dim=out_dim, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(output_dim=1, kernel_initializer='normal'))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        \n",
    "        start = time.time()\n",
    "        model.fit(X, y, \n",
    "        verbose=verb, \n",
    "        nb_epoch=num_epochs, \n",
    "        batch_size=bs,\n",
    "        validation_data=(X_validation, y_validation))\n",
    "        end = time.time()\n",
    "        \n",
    "#         pred = model.predict(X_validation)\n",
    "#         accuracy = rmspe(y_validation, pred)\n",
    "        \n",
    "        nic = pd.read_csv('submission2.csv')\n",
    "        nic_pred = nic.Sales\n",
    "        \n",
    "        pred_test = model.predict(X_test)\n",
    "        \n",
    "        for i in range (X_test.shape[0]) :\n",
    "            if df_test_store['Customers'].iloc[i] == 0 :\n",
    "                pred_test[i] = 0\n",
    "                \n",
    "        accuracy = rmspe(nic_pred, pred_test)\n",
    "        \n",
    "        print('---------//---------')\n",
    "        print('output     ',out_dim)\n",
    "        print('batch size ',bs)\n",
    "        print('epochs ',num_epochs)\n",
    "#         print('time taken ',end-start)\n",
    "        print(\"RMSPE is \",accuracy)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(X_test)\n",
    "pred_test = np.reshape(pred_test, pred_test.shape[0])\n",
    "submission = pd.DataFrame()\n",
    "submission['Sales'] = pred_test\n",
    "cols = ['Id','Sales']\n",
    "submission['Id'] = submission.index + 1\n",
    "submission = submission[cols]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
